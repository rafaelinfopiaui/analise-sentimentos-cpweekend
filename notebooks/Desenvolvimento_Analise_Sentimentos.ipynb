{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrmLAwjma0Q9hNVXPYcRVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaelinfopiaui/analise-sentimentos-cpweekend/blob/main/notebooks/Desenvolvimento_Analise_Sentimentos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ An√°lise de Sentimentos para a Campus Party Weekend Piau√≠ 2025\n",
        "\n",
        "**Objetivo:** Desenvolver um prot√≥tipo de Machine Learning capaz de classificar o sentimento de textos em portugu√™s (positivo, negativo, neutro) a partir de dados coletados de redes sociais.\n",
        "\n",
        "**Projeto de Extens√£o:** Engenharia de Computa√ß√£o com IA - UNI-CET\n",
        "\n",
        "**Equipe de Desenvolvedores:**\n",
        "* Rafael Oliveira\n",
        "* Ailton Medeiros\n",
        "* Lais Eul√°lio\n",
        "* Ant√¥nio Wilker\n",
        "* Isaac Arag√£o\n",
        "* Paula Iranda\n",
        "\n",
        "**Docente Orientador:** Prof. Dr. Artur Felipe da Silva Veloso\n",
        "\n",
        "**Etapas deste Notebook:**\n",
        "1.  **Configura√ß√£o do Ambiente:** Conex√£o com o Google Drive e importa√ß√£o de bibliotecas.\n",
        "2.  **Carga e An√°lise dos Dados:** Carregar o dataset e fazer uma an√°lise explorat√≥ria inicial.\n",
        "3.  **Pr√©-Processamento:** Limpeza e prepara√ß√£o dos textos para o modelo.\n",
        "4.  **Treinamento do Modelo:** Constru√ß√£o, treino e avalia√ß√£o de um modelo de classifica√ß√£o.\n",
        "5.  **Salvamento do Modelo:** Exportar o modelo treinado para ser usado na aplica√ß√£o com Streamlit."
      ],
      "metadata": {
        "id": "vQHhp1xurovh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configura√ß√£o do Ambiente\n",
        "\n",
        "Nesta primeira etapa, a equipe ir√° conectar o Colab ao Google Drive para acessar os dados, importar todas as bibliotecas que ser√£o utilizadas no projeto e definir os caminhos dos arquivos para manter o c√≥digo organizado e acess√≠vel a todos."
      ],
      "metadata": {
        "id": "QuCMDPHGsOWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 - Conectar ao Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1.2 - Importar bibliotecas essenciais\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import joblib\n",
        "\n",
        "# 1.3 - Baixar pacotes do NLTK (s√≥ precisa na primeira vez)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 1.4 - Importar m√≥dulos do Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# --- ATEN√á√ÉO: CONFIGURAR CAMINHOS DO PROJETO AQUI ---\n",
        "# Garanta que o nome da pasta principal seja o mesmo para toda a equipe.\n",
        "PASTA_PROJETO = '/content/drive/MyDrive/Projeto_CampusParty_Sentimentos'\n",
        "CAMINHO_DADOS_BRUTOS = f'{PASTA_PROJETO}/data/dataset_bruto.csv'\n",
        "CAMINHO_DADOS_LIMPOS = f'{PASTA_PROJETO}/data/dados_limpos.csv'\n",
        "CAMINHO_MODELO = f'{PASTA_PROJETO}/saved_models/modelo_sentimento.joblib'\n",
        "\n",
        "print(\"Ambiente configurado com sucesso!\")"
      ],
      "metadata": {
        "id": "D64IZi4bsPEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Carga e An√°lise Explorat√≥ria dos Dados (EDA)\n",
        "\n",
        "Nesta etapa, a equipe carrega o dataset e realiza uma verifica√ß√£o r√°pida para entender sua estrutura, a quantidade de dados, a presen√ßa de valores nulos e a distribui√ß√£o dos sentimentos."
      ],
      "metadata": {
        "id": "ZCwe-TDfsSla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 - Carregar o dataset\n",
        "try:\n",
        "    df = pd.read_csv(CAMINHO_DADOS_BRUTOS)\n",
        "    print(\"Dataset carregado com sucesso!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Erro: Arquivo n√£o encontrado em '{CAMINHO_DADOS_BRUTOS}'. Verifiquem o caminho e o nome do arquivo no Drive.\")\n",
        "\n",
        "# 2.2 - Visualizar as primeiras linhas\n",
        "print(\"Primeiras 5 linhas do dataset:\")\n",
        "display(df.head())\n",
        "\n",
        "# 2.3 - Verificar informa√ß√µes gerais\n",
        "print(\"\\nInforma√ß√µes do DataFrame:\")\n",
        "df.info()\n",
        "\n",
        "# 2.4 - Verificar a distribui√ß√£o das classes de sentimento\n",
        "# (A equipe deve substituir 'sentimento' pelo nome real da coluna de classifica√ß√£o)\n",
        "print(\"\\nDistribui√ß√£o dos sentimentos:\")\n",
        "print(df['sentimento'].value_counts())"
      ],
      "metadata": {
        "id": "Fwt9iK8isUp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Pr√©-processamento e Limpeza dos Textos\n",
        "\n",
        "Esta √© uma das etapas mais cr√≠ticas. A equipe definiu uma fun√ß√£o de limpeza para padronizar o tratamento dos textos de redes sociais, que s√£o inerentemente \"sujos\" (cont√™m URLs, men√ß√µes, etc.). Isso garante que o modelo foque apenas nas palavras que carregam o sentimento."
      ],
      "metadata": {
        "id": "bmJGNOP5sWzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 - Definir a fun√ß√£o de limpeza de texto\n",
        "def limpar_texto(texto):\n",
        "    # Converter para min√∫sculas\n",
        "    texto = texto.lower()\n",
        "    # Remover URLs\n",
        "    texto = re.sub(r'https?://\\S+|www\\.\\S+', '', texto)\n",
        "    # Remover men√ß√µes (@) e hashtags (#)\n",
        "    texto = re.sub(r'@\\w+|#\\w+', '', texto)\n",
        "    # Remover caracteres n√£o-alfab√©ticos (mant√©m letras e espa√ßos)\n",
        "    texto = re.sub(r'[^a-z\\s]', '', texto)\n",
        "    # Tokeniza√ß√£o (dividir em palavras)\n",
        "    tokens = nltk.word_tokenize(texto)\n",
        "    # Remover stopwords\n",
        "    stopwords_pt = nltk.corpus.stopwords.words('portuguese')\n",
        "    tokens_limpos = [palavra for palavra in tokens if palavra not in stopwords_pt]\n",
        "    # Juntar os tokens de volta em uma string\n",
        "    return ' '.join(tokens_limpos)\n",
        "\n",
        "# 3.2 - Aplicar a fun√ß√£o de limpeza na coluna de texto\n",
        "# (A equipe deve substituir 'texto' pelo nome real da coluna de texto)\n",
        "df['texto_limpo'] = df['texto'].apply(limpar_texto)\n",
        "\n",
        "# 3.3 - Visualizar o resultado (antes e depois)\n",
        "print(\"Compara√ß√£o do texto original vs. texto limpo:\")\n",
        "display(df[['texto', 'texto_limpo']].head())\n",
        "\n",
        "# 3.4 - Salvar o DataFrame limpo\n",
        "df.to_csv(CAMINHO_DADOS_LIMPOS, index=False)\n",
        "print(f\"\\nDataFrame limpo salvo em: {CAMINHO_DADOS_LIMPOS}\")"
      ],
      "metadata": {
        "id": "ziVN5dk0sYnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Constru√ß√£o, Treinamento e Avalia√ß√£o do Modelo\n",
        "\n",
        "M√£os √† obra, equipe! Agora vamos:\n",
        "1.  Dividir os dados em conjuntos de **treino** e **teste**.\n",
        "2.  Construir um **`Pipeline`** que automatiza a vetoriza√ß√£o do texto (TF-IDF) e a classifica√ß√£o (Regress√£o Log√≠stica).\n",
        "3.  **Treinar** o modelo com os dados de treino.\n",
        "4.  **Avaliar** a performance coletivamente com os dados de teste."
      ],
      "metadata": {
        "id": "KenQxuaAsalk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 - Definir X (features) e y (target)\n",
        "X = df['texto_limpo']\n",
        "y = df['sentimento'] # A equipe deve substituir 'sentimento' se o nome da coluna for diferente\n",
        "\n",
        "# 4.2 - Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 4.3 - Construir o Pipeline do modelo\n",
        "modelo_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "# 4.4 - Treinar o modelo\n",
        "print(\"Iniciando o treinamento do modelo...\")\n",
        "modelo_pipeline.fit(X_train, y_train)\n",
        "print(\"Treinamento conclu√≠do!\")\n",
        "\n",
        "# 4.5 - Fazer previs√µes no conjunto de teste\n",
        "y_pred = modelo_pipeline.predict(X_test)\n",
        "\n",
        "# 4.6 - Avaliar o modelo\n",
        "acuracia = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAcur√°cia do modelo: {acuracia:.4f}\")\n",
        "\n",
        "print(\"\\nRelat√≥rio de Classifica√ß√£o:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "bg8uE448schK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Salvando o Pipeline Final\n",
        "\n",
        "Miss√£o cumprida, equipe! O modelo est√° treinado e avaliado. O √∫ltimo passo neste notebook √© salvar o `Pipeline` completo (vetorizador + classificador) em um arquivo `.joblib`. Este arquivo ser√° o \"c√©rebro\" da nossa aplica√ß√£o com Streamlit."
      ],
      "metadata": {
        "id": "5tIdccm5sejf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1 - Salvar o pipeline treinado\n",
        "joblib.dump(modelo_pipeline, CAMINHO_MODELO)\n",
        "\n",
        "print(f\"Modelo salvo com sucesso em: {CAMINHO_MODELO}\")\n",
        "print(\"\\nPr√≥ximo passo: A equipe ir√° baixar este arquivo do Google Drive para construir a aplica√ß√£o com Streamlit!\")"
      ],
      "metadata": {
        "id": "FOCrDZ3psgAs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}