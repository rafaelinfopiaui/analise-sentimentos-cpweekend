import streamlit as st
import pickle # AGORA USAMOS PICKLE
import re
import numpy as np # Necess√°rio para o bypass do pickle
from nltk.corpus import stopwords
import nltk

# 1. Configura√ß√£o do Streamlit (DEVE SER O PRIMEIRO COMANDO ST)
st.set_page_config(page_title="An√°lise de Sentimentos CPWPI", layout="centered")

# --- 2. Configura√ß√µes Iniciais e Carregamento dos Componentes ---

# Caminhos para os arquivos separados, AGORA .pkl
TFIDF_PATH = 'saved_models/tfidf_vectorizer.pkl'
CLF_PATH = 'saved_models/logistic_model.pkl'

# Garante que 'stopwords' esteja acess√≠vel
try:
    stopwords_pt = stopwords.words('portuguese')
except LookupError:
    nltk.download('stopwords')
    stopwords_pt = stopwords.words('portuguese')


# Fun√ß√£o para manipular a falha de compatibilidade do numpy/pickle
class CustomUnpickler(pickle.Unpickler):
    """
    Solu√ß√£o para o erro ModuleNotFoundError: No module named 'numpy_core'
    e outras falhas de serializa√ß√£o entre Colab e WSL.
    For√ßa o carregamento das classes do numpy do m√≥dulo base.
    """
    def find_class(self, module, name):
        if module == 'numpy.core.multiarray':
            return np.__dict__[name]
        if module == 'numpy.core.umath':
            return np.__dict__[name]
        if module == 'sklearn.feature_extraction.text':
            import sklearn.feature_extraction.text
            return sklearn.feature_extraction.text.__dict__[name]
        return super().find_class(module, name)

# Fun√ß√£o para carregar os dois componentes (Vetorizador e Classificador)
@st.cache_resource 
def load_components():
    try:
        # Carrega os dois componentes usando PICKLE com o bypass
        with open(TFIDF_PATH, 'rb') as f:
            # Usa o CustomUnpickler para carregar o vetorizador
            vectorizer = CustomUnpickler(f).load() 
        
        with open(CLF_PATH, 'rb') as f:
            # Usa o CustomUnpickler para carregar o classificador
            classifier = CustomUnpickler(f).load() 
        
        return vectorizer, classifier
    except FileNotFoundError:
        return None, None
    except Exception as e:
        # Se falhar mesmo com o bypass, imprime o erro final e retorna None
        print(f"Erro Cr√≠tico de Carregamento com Bypass: {e}")
        return None, None

vectorizer, classifier = load_components()

# --- 3. Fun√ß√£o de Pr√©-processamento e Predi√ß√£o ---

# Fun√ß√£o de Pr√©-processamento (Final e Robusta - Sincronizada com o Notebook)
def limpar_texto(texto):
    if not isinstance(texto, str):
        return ""
    
    texto = texto.lower()
    # Remove URLs, men√ß√µes, hashtags
    texto = re.sub(r'https?://\S+|www\.\S+', '', texto) 
    texto = re.sub(r'@\w+|#\w+', '', texto)             
    
    # Remove pontua√ß√£o (CRUCIAL para o .split() funcionar como tokenizador)
    texto = re.sub(r'[^\w\s]', '', texto) 
    
    # Tokeniza√ß√£o Simples (BYPASS DO ERRO 'punkt')
    tokens = texto.split()
    
    # Remover stopwords
    stopwords_pt = stopwords.words('portuguese')
    tokens_limpos = [palavra for palavra in tokens if palavra not in stopwords_pt]
    
    return ' '.join(tokens_limpos)

# Mapeamento de Sentimento
SENTIMENT_MAP = {
    '0': 'Negativo',
    '1': 'Positivo',
    'negativo': 'Negativo',
    'positivo': 'Positivo',
}

# Fun√ß√£o de Predi√ß√£o (L√≥gica de Componentes Separados)
def predict_sentiment(text):
    if vectorizer is None or classifier is None:
        return "MODEL_ERROR"
        
    processed_text = limpar_texto(text)
    
    # 2. Vetoriza√ß√£o: Transforma o texto limpo usando o vetorizador
    X_vectorized = vectorizer.transform([processed_text])
    
    # 3. Classifica√ß√£o: Aplica o classificador no vetor
    prediction = classifier.predict(X_vectorized)[0]
    
    # Mapeia o resultado
    return SENTIMENT_MAP.get(str(prediction).lower(), str(prediction))

# --- 4. Interface Principal do Streamlit ---

st.title("üß† Classificador de Sentimentos em Portugu√™s")
st.subheader("Projeto de Extens√£o: CP Weekend Piau√≠ 2025")


# Verifica e exibe o status do modelo (AP√ìS st.set_page_config e st.title)
if vectorizer is None or classifier is None:
    st.error(f"Erro: Arquivos do modelo n√£o encontrados. Verifique se {TFIDF_PATH} e {CLF_PATH} est√£o na pasta 'saved_models/' e t√™m a extens√£o .pkl.")
    st.warning("√â necess√°rio fazer a √∫ltima execu√ß√£o do Notebook para gerar os dois arquivos **.pkl**.")
    st.stop()
else:
    st.sidebar.success("Modelo de ML carregado com sucesso!")


st.markdown("Esta aplica√ß√£o utiliza o **Pipeline** de Machine Learning treinado para classificar o sentimento.")

text_input = st.text_area("Insira um texto para an√°lise:", "A Campus Party Weekend Piau√≠ √© um evento incr√≠vel, mal posso esperar!", height=150)

# Bot√£o de Predi√ß√£o
if st.button("Analisar Sentimento"):
    if text_input:
        with st.spinner('Analisando o texto...'):
            sentiment = predict_sentiment(text_input)
            
            st.markdown("---")
            st.write("### Resultado da An√°lise:")
            
            # Formatando o output
            if sentiment == "Positivo":
                st.success(f"O sentimento √©: **{sentiment}** üéâ")
            elif sentiment == "Negativo":
                st.error(f"O sentimento √©: **{sentiment}** üôÅ")
            elif sentiment == "MODEL_ERROR":
                st.error("Erro no Pipeline. Verifique o console para detalhes.")
            else:
                st.info(f"O sentimento √©: **{sentiment}**")
            
            st.markdown("---")
    else:
        st.warning("Por favor, insira um texto para realizar a an√°lise.")

# Rodap√©/Sidebar
st.sidebar.markdown("---")
st.sidebar.markdown("### Apoio Institucional")

# Inclus√£o das Imagens na Sidebar
st.sidebar.image("uploaded:unicet_white.png-ad624093-8365-4be1-82c3-885eca156d1d", use_column_width=True)
st.sidebar.image("uploaded:ENG-CIA logo.jpg-0f996b2d-fba4-498c-9ebf-828b42ca9be2", use_column_width=True)
st.sidebar.image("uploaded:CPWeekend_Piaui.png-2319df34-36a1-43df-ac89-62d347622253", use_column_width=True)

st.sidebar.markdown("---")
st.sidebar.markdown(f"**Desenvolvimento:** Equipe de Extens√£o UNI-CET")